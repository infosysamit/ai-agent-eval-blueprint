{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345a359f-59c3-4be2-a37c-41fae7479bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "Q: Define TTFT.\n",
      "\n",
      "Prediction:\n",
      "**TTFT** stands for **Time To First Token**.\n",
      "\n",
      "It's a performance metric that measures the latency between when a user submits a request to a language model (LLM) or AI system and when the first token of the response is generated and returned to the user.\n",
      "\n",
      "TTFT is important because:\n",
      "- It affects perceived responsiveness and user experience\n",
      "- Lower TTFT means users see output starting sooner\n",
      "- It's particularly critical for streaming responses where users want immediate feedback\n",
      "\n",
      "TTFT is influenced by factors like model size, prompt length, server load, and infrastructure efficiency.\n",
      "\n",
      "Helpfulness: 1 — Let me analyze whether this submission meets the helpfulness criterion by evaluating if it is practically useful and clear.\n",
      "\n",
      "**Step-by-step reasoning:**\n",
      "\n",
      "1. **Does it define the term clearly?**\n",
      "   - Yes, it explicitly states \"TTFT stands for Time To First Token\"\n",
      "   - The definition is straightforward and unambiguous\n",
      "\n",
      "2. **Does it explain what the term means in practical terms?**\n",
      "   - Yes, it describes it as \"a performance metric that measures the latency between when a user submits a request to a language model (LLM) or AI system and when the first token of the response is generated\"\n",
      "   - This provides concrete understanding of what is being measured\n",
      "\n",
      "3. **Does it provide context for why this matters?**\n",
      "   - Yes, it explains the importance through multiple points:\n",
      "     - Affects user experience\n",
      "     - Lower TTFT means faster perceived response\n",
      "     - Critical for streaming responses\n",
      "   - This helps the reader understand practical relevance\n",
      "\n",
      "4. **Is the information organized clearly?**\n",
      "   - Yes, it follows a logical structure: definition → explanation → importance → influencing factors\n",
      "   - Uses bullet points for easy scanning\n",
      "   - Well-formatted with bold text for the acronym\n",
      "\n",
      "5. **Does it provide additional useful information?**\n",
      "   - Yes, it mentions factors that influence TTFT (model size, prompt length, server load, infrastructure)\n",
      "   - This adds practical value for someone trying to understand or optimize TTFT\n",
      "\n",
      "6. **Is the language accessible?**\n",
      "   - Yes, the explanation avoids unnecessary jargon while remaining technically accurate\n",
      "   - Clear and concise\n",
      "\n",
      "The submission is both practically useful (provides actionable understanding of the concept) and clear (well-organized, easy to understand).\n",
      "\n",
      "Y\n",
      "Correctness: 1 — Let me analyze whether the submission meets the correctness criterion by comparing it to the reference answer.\n",
      "\n",
      "**Step-by-step reasoning:**\n",
      "\n",
      "1. **Core Definition Check:**\n",
      "   - Reference states: \"Time-to-first-token: latency from request start to first token\"\n",
      "   - Submission states: \"measures the latency between when a user submits a request to a language model (LLM) or AI system and when the first token of the response is generated and returned to the user\"\n",
      "   - These definitions align - both describe TTFT as the latency/time from when a request starts until the first token is received.\n",
      "\n",
      "2. **Acronym Expansion:**\n",
      "   - Reference implies: \"Time-to-first-token\" (hyphenated)\n",
      "   - Submission states: \"Time To First Token\" (no hyphens)\n",
      "   - This is a minor stylistic difference but conveys the same meaning.\n",
      "\n",
      "3. **Additional Information:**\n",
      "   - The submission provides extra context about why TTFT is important, what factors influence it, and its relevance to user experience\n",
      "   - The reference doesn't contradict any of this additional information\n",
      "   - Adding correct supplementary information doesn't make an answer incorrect\n",
      "\n",
      "4. **Accuracy of Core Concept:**\n",
      "   - Both answers correctly identify TTFT as a latency metric\n",
      "   - Both correctly identify it measures from request start to first token\n",
      "   - The submission's additional details about it being used in LLM/AI contexts are accurate and relevant\n",
      "\n",
      "**Conclusion:**\n",
      "The submission correctly defines TTFT in alignment with the reference answer. The core definition matches, and the additional explanatory information is accurate and helpful rather than incorrect or contradictory.\n",
      "\n",
      "Y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimal, one-sample evaluation with Claude + LangChain, that you should try\n",
    "# Goal of evaluation: show BOTH reference-free (helpfulness) and reference-aware (correctness) scoring.\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# 1) Pick a stable, versioned Claude model \n",
    "llm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\", temperature=0)\n",
    "\n",
    "# 2) One gold sample keeps the snippet readable for the paper.\n",
    "item = {\n",
    "    \"question\": \"Define TTFT.\",\n",
    "    \"reference\": \"Time-to-first-token: latency from request start to first token.\"\n",
    "}\n",
    "\n",
    "# 3) System under test: a deterministic call to Claude.\n",
    "def predict(q: str) -> str:\n",
    "    return llm.invoke([(\"system\", \"Answer concisely.\"), (\"human\", q)]).content\n",
    "\n",
    "pred = predict(item[\"question\"])\n",
    "\n",
    "# 4) Evaluators:\n",
    "#    - \"criteria\".. reference-free (UX-style qualities like helpfulness).\n",
    "#    - \"labeled_criteria\".. reference-aware (fact checks vs. the reference).\n",
    "crit_eval = load_evaluator(\n",
    "    \"criteria\",\n",
    "    llm=llm,\n",
    "    criteria={\"helpfulness\": \"Is the answer practically useful and clear?\"}\n",
    ")\n",
    "lab_eval = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    llm=llm,\n",
    "    criteria={\"correctness\": \"Is the answer correct given the reference?\"}\n",
    ")\n",
    "\n",
    "# 5) Get scores (+ rationales)\n",
    "res_help = crit_eval.evaluate_strings(prediction=pred, input=item[\"question\"])\n",
    "res_corr = lab_eval.evaluate_strings(\n",
    "    prediction=pred, input=item[\"question\"], reference=item[\"reference\"]\n",
    ")\n",
    "\n",
    "help_score = res_help.get(\"score\")\n",
    "help_note  = res_help.get(\"reasoning\") or res_help.get(\"explanation\")\n",
    "corr_score = res_corr.get(\"score\")\n",
    "corr_note  = res_corr.get(\"reasoning\") or res_corr.get(\"explanation\")\n",
    "\n",
    "# 6) Printout \n",
    "print(\n",
    "    f\"\\n{'='*64}\\n\"\n",
    "    f\"Q: {item['question']}\\n\\n\"\n",
    "    f\"Prediction:\\n{pred}\\n\\n\"\n",
    "    f\"Helpfulness: {help_score} — {help_note}\\n\"\n",
    "    f\"Correctness: {corr_score} — {corr_note}\\n\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b0753-bcdc-4b52-b212-423c948ec8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2e53a-062d-4003-ac96-507051cc85c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
